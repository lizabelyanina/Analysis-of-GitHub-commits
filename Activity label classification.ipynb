{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3d6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flask.repo_parser import process_github_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d09b5",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "source: open github repos, that follow the conventional commit standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f16f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfpm = process_github_repo('https://github.com/goreleaser/nfpm', conventional_commits=True)\n",
    "df_yargs = process_github_repo('https://github.com/yargs/yargs', conventional_commits=True)\n",
    "df_istanbuljs = process_github_repo('https://github.com/istanbuljs/istanbuljs', conventional_commits=True)\n",
    "df_uportal = process_github_repo('https://github.com/uPortal-Attic/uportal-home', conventional_commits=True)\n",
    "df_uportal2 = process_github_repo('https://github.com/uPortal-Attic/uportal-app-framework', conventional_commits=True)\n",
    "df_monica = process_github_repo('https://github.com/monicahq/monica', conventional_commits=True)\n",
    "df_changelogguru = pd.read_csv('/Users/elizavetabelanina/Analysis-of-GitHub-commits/NN training data/changeloguru.csv')\n",
    "df_massivejs = process_github_repo('https://github.com/dmfay/massive-js', conventional_commits=True)\n",
    "df_scrollutility = process_github_repo('https://github.com/LeDDGroup/scroll-utility', conventional_commits=True)\n",
    "df_blaze = process_github_repo('https://github.com/BlazeUI/blaze', conventional_commits=True)\n",
    "df_diffparse = process_github_repo('https://github.com/danielduarte/diffparse', conventional_commits=True)\n",
    "df_diffsplit = process_github_repo('https://github.com/danielduarte/diffsplit', conventional_commits=True)\n",
    "df_firestarter = process_github_repo('https://github.com/HunWalk/yii2-basic-firestarter', conventional_commits=True)\n",
    "df_resume = process_github_repo('https://github.com/dcyou/resume', conventional_commits=True)\n",
    "df_belajar = process_github_repo('https://github.com/belajarpythoncom/belajarpython.com', conventional_commits=True)\n",
    "df_electron = process_github_repo('https://github.com/electron/electron', conventional_commits=True)\n",
    "df_tina = process_github_repo('https://github.com/tinacms/tinacms', conventional_commits=True)\n",
    "df_uno = process_github_repo('https://github.com/unoplatform/uno', conventional_commits=True)\n",
    "df_jenkins = process_github_repo('https://github.com/jenkins-x/jx', conventional_commits=True)\n",
    "df_gearlock = process_github_repo('https://github.com/axonasif/gearlock', conventional_commits=True)\n",
    "df_codecamp = process_github_repo('https://github.com/freeCodeCamp/freeCodeCamp', conventional_commits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bfbc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes and their corresponding GitHub sources\n",
    "dataframes = [\n",
    "    (df_nfpm, 'https://github.com/goreleaser/nfpm'),\n",
    "    (df_yargs, 'https://github.com/yargs/yargs'),\n",
    "    (df_istanbuljs, 'https://github.com/istanbuljs/istanbuljs'),\n",
    "    (df_uportal, 'https://github.com/uPortal-Attic/uportal-home'),\n",
    "    (df_uportal2, 'https://github.com/uPortal-Attic/uportal-app-framework'),\n",
    "    (df_monica, 'https://github.com/monicahq/monica'),\n",
    "    (df_changelogguru, 'https://github.com/haunt98/changeloguru'),\n",
    "    (df_massivejs, 'https://github.com/dmfay/massive-js'),\n",
    "    (df_scrollutility, 'https://github.com/LeDDGroup/scroll-utility'),\n",
    "    (df_blaze, 'https://github.com/BlazeUI/blaze'),\n",
    "    (df_diffparse, 'https://github.com/danielduarte/diffparse'),\n",
    "    (df_diffsplit, 'https://github.com/danielduarte/diffsplit'),\n",
    "    (df_firestarter, 'https://github.com/HunWalk/yii2-basic-firestarter'),\n",
    "    (df_resume, 'https://github.com/dcyou/resume'),\n",
    "    (df_belajar, 'https://github.com/belajarpythoncom/belajarpython.com'),\n",
    "    (df_electron, 'https://github.com/electron/electron'),\n",
    "    (df_tina, 'https://github.com/tinacms/tinacms'),\n",
    "    (df_uno, 'https://github.com/unoplatform/uno'),\n",
    "    (df_jenkins, 'https://github.com/jenkins-x/jx'),\n",
    "    (df_gearlock, 'https://github.com/axonasif/gearlock'),\n",
    "    (df_codecamp, 'https://github.com/freeCodeCamp/freeCodeCamp')\n",
    "]\n",
    "\n",
    "# Concatenate dataframes and add the source column\n",
    "df_group = pd.concat(\n",
    "    [df.assign(source=source) for df, source in dataframes],\n",
    "    ignore_index=True\n",
    ").sample(frac=1, random_state=1)\n",
    "\n",
    "# Display the concatenated dataframe\n",
    "df_group.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abec7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group.to_csv('df_group_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47b4b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_angular = pd.read_csv('/Users/elizavetabelanina/Analysis-of-GitHub-commits/data/angular_github_commits.csv')\n",
    "concatenated_df = pd.concat([df_group, df_angular], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a341016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_df.to_csv('final_unpreprocessed.csv')\n",
    "# concatenated_df = pd.read_csv('final_unpreprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7d5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_text_before_parenthesis(commit_message):\n",
    "    # Find the index of the first '(' and return the substring before it\n",
    "    index = commit_message.find('(')\n",
    "    if index != -1:\n",
    "        return commit_message[:index].strip()  # Return text before '(' and strip whitespace\n",
    "    return commit_message.strip()  # Return the original message if '(' is not found\n",
    "\n",
    "def parse_commit_message(commit_message):\n",
    "    # Updated regex pattern to capture everything before the first ':' as type\n",
    "    pattern = r'^\\s*(?P<type>[^:]+):\\s*(?P<rest>.*)$'\n",
    "    \n",
    "    # Use re.DOTALL to ensure '.' matches newlines as well\n",
    "    match = re.match(pattern, commit_message, re.DOTALL)\n",
    "    \n",
    "    if commit_message.lower().startswith(\"merge\"):\n",
    "        return \"merge\", commit_message.strip()\n",
    "    elif commit_message.lower().startswith(\"revert\"):\n",
    "            return \"revert\", commit_message.strip()  # Return \"merge\" for merge messages\n",
    "    \n",
    "    if match:\n",
    "        commit_type = match.group('type').strip()\n",
    "        rest_of_message = match.group('rest').strip()\n",
    "        return get_text_before_parenthesis(commit_type).lower(), rest_of_message\n",
    "    else:\n",
    "        return 'undefined', commit_message.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2beb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_commits(commits):\n",
    "    parsed_messages = []\n",
    "    for msg in commits:\n",
    "        commit_type, rest = parse_commit_message(msg)\n",
    "        parsed_messages.append((commit_type, rest))\n",
    "    return pd.DataFrame(parsed_messages, columns=['commit_type', 'commit_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = label_commits(concatenated_df.commit_message)\n",
    "labeled_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_commit_types = ['refactor', 'fix', 'feat', 'chore', 'docs', 'test', 'style',\n",
    "       'revert', 'merge', 'ci', 'build', 'perf',  'release']\n",
    "len(valid_commit_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for commit types\n",
    "#value_counts = labeled_dataset.commit_type.value_counts()\n",
    "\n",
    "# Get the commit types that have a count of 10 or more\n",
    "#valid_commit_types = value_counts[(value_counts >= 10) & (value_counts.index != 'undefined')].index\n",
    "\n",
    "# Filter the labeled_dataset to keep only those commit types\n",
    "filtered_labeled_dataset = labeled_dataset[labeled_dataset.commit_type.isin(valid_commit_types)].reset_index(drop=True).dropna()\n",
    "\n",
    "# Display the filtered dataset\n",
    "print(filtered_labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8bdd2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c06420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = filtered_labeled_dataset.commit_type\n",
    "label_encoder = LabelEncoder()\n",
    "integer_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01233f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs but keep their context\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\[|\\]|\\(|\\)', '', text)\n",
    "    \n",
    "    # Remove special characters and punctuation, but keep some structure\n",
    "    text = re.sub(r'[^\\w\\s\\-]', ' ', text)\n",
    "    \n",
    "    # Replace hyphens with spaces\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "388569d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_raw = filtered_labeled_dataset.commit_message\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e6c74",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c177f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming preprocessed_texts and labels are already defined\n",
    "\n",
    "# Convert labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "integer_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# First, split the data into training and temp (which will be further split into validation and testing)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    preprocessed_texts, integer_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Now split the temp dataset into validation and testing\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextClassificationDataset(train_encodings, train_labels)\n",
    "val_dataset = TextClassificationDataset(val_encodings, val_labels)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Set up the model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "model.to(device)\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,  # Decreased batch size\n",
    "    per_device_eval_batch_size=32,    # Adjusted eval batch size\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=2,  # Use gradient accumulation\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Save the label encoder\n",
    "import joblib\n",
    "joblib.dump(label_encoder, './fine_tuned_model/label_encoder.joblib')\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Function for making predictions\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = outputs.logits.softmax(dim=-1)\n",
    "    predicted_class = probs.argmax().item()\n",
    "    return label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "# Example usage\n",
    "new_text = \"Your new text here\"\n",
    "predicted_label = predict(new_text)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results.get('eval_accuracy', 'Accuracy not computed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc29907",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06f809f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix: np.ndarray, labels: List[str], title: str = \"Confusion Matrix\") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a styled confusion matrix plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.set(font_scale=1.2)\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                         xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title, pad=20)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_metrics_comparison(class_metrics: Dict, metric_name: str = \"f1-score\") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a bar plot comparing a specific metric across classes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = [metrics[metric_name] for metrics in class_metrics.values()]\n",
    "    classes = list(class_metrics.keys())\n",
    "    \n",
    "    plt.bar(classes, metrics)\n",
    "    plt.title(f'{metric_name.title()} by Class')\n",
    "    plt.ylabel(metric_name.title())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_precision_recall_curve(class_metrics: Dict) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a scatter plot of precision vs recall for each class.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    precisions = [metrics['precision'] for metrics in class_metrics.values()]\n",
    "    recalls = [metrics['recall'] for metrics in class_metrics.values()]\n",
    "    classes = list(class_metrics.keys())\n",
    "    \n",
    "    plt.scatter(recalls, precisions)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        plt.annotate(class_name, (recalls[i], precisions[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title('Precision vs Recall by Class')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def save_visualizations(metrics: Dict, output_dir: str = \"evaluation_plots\"):\n",
    "    \"\"\"\n",
    "    Save all visualization plots to files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    conf_matrix_fig = plot_confusion_matrix(\n",
    "        metrics['confusion_matrix'],\n",
    "        labels=list(metrics['class_metrics'].keys())\n",
    "    )\n",
    "    conf_matrix_fig.savefig(f'{output_dir}/confusion_matrix.pdf', bbox_inches='tight')\n",
    "    \n",
    "    # Save F1-score comparison plot\n",
    "    f1_fig = plot_metrics_comparison(metrics['class_metrics'], \"f1-score\")\n",
    "    f1_fig.savefig(f'{output_dir}/f1_scores.pdf', bbox_inches='tight')\n",
    "    \n",
    "    # Save precision-recall plot\n",
    "    pr_fig = plot_precision_recall_curve(metrics['class_metrics'])\n",
    "    pr_fig.savefig(f'{output_dir}/precision_recall.pdf', bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "def evaluate_model(y_true: List[str], y_pred: List[str], labels: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate the model performance using multiple metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'classification_report': classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    class_metrics = {}\n",
    "    report = metrics['classification_report']\n",
    "    for label in labels:\n",
    "        class_metrics[label] = {\n",
    "            'precision': report[label]['precision'],\n",
    "            'recall': report[label]['recall'],\n",
    "            'f1-score': report[label]['f1-score'],\n",
    "            'support': report[label]['support']\n",
    "        }\n",
    "    metrics['class_metrics'] = class_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def generate_evaluation_report(metrics: Dict, model_name: str = \"DistilBERT\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a markdown report from evaluation metrics.\n",
    "    \"\"\"\n",
    "    report = f\"# Model Evaluation Report: {model_name} for Commit Classification\\n\\n\"\n",
    "    \n",
    "    report += \"## Overall Performance\\n\\n\"\n",
    "    report += f\"- **Accuracy**: {metrics['accuracy']:.4f}\\n\"\n",
    "    report += f\"- **Macro Avg F1-Score**: {metrics['classification_report']['macro avg']['f1-score']:.4f}\\n\"\n",
    "    report += f\"- **Weighted Avg F1-Score**: {metrics['classification_report']['weighted avg']['f1-score']:.4f}\\n\\n\"\n",
    "    \n",
    "    report += \"## Per-class Performance\\n\\n\"\n",
    "    report += \"| Class | Precision | Recall | F1-Score | Support |\\n\"\n",
    "    report += \"|-------|-----------|---------|-----------|----------|\\n\"\n",
    "    \n",
    "    for label, metrics_dict in metrics['class_metrics'].items():\n",
    "        report += f\"| {label} | {metrics_dict['precision']:.3f} | {metrics_dict['recall']:.3f} | \"\n",
    "        report += f\"{metrics_dict['f1-score']:.3f} | {metrics_dict['support']} |\\n\"\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59264b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_labels\n",
    "y_pred = [predict(text) for text in test_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47069637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluation_metrics = evaluate_model(label_encoder.inverse_transform(test_labels), y_pred, labels.unique())\n",
    "    \n",
    "# Generate report\n",
    "report = generate_evaluation_report(evaluation_metrics)\n",
    "\n",
    "save_visualizations(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a30cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(label_encoder.inverse_transform(test_labels), y_pred, labels = labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
